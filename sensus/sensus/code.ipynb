{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a1d61f5e914c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Initializing the weights and the biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 'sizes' is a list, with each element being the number of neurons in a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-a1d61f5e914c>\u001b[0m in \u001b[0;36mNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         is the same with respect to biases. \"\"\"\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mgrad_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    \n",
    "    # Initializing the weights and the biases\n",
    "    \n",
    "    def __init__(self, sizes): # 'sizes' is a list, with each element being the number of neurons in a layer\n",
    "        self.n_layers = len(sizes)\n",
    "        self.sizes = sizes \n",
    "        self.weights = [np.random.randn(j, k) for k, j in zip(sizes[:-1], sizes[1:])] # j and k were swapped deliberately\n",
    "        self.biases = [np.random.randn(j, 1) for j in sizes[1:]] # a vector of initial biases, excluding the input layer\n",
    "    \n",
    "    # The activation function that computes the input of a neuron as the weighted sum of the previous neurons' outputs, \n",
    "    # the weights and the biases\n",
    "    \n",
    "    def activation(self, a): # 'a' is a vector of outputs from the previous layer (i.e., input of the current layer)\n",
    "        \"\"\" The activation of a neuron \"\"\"\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b \n",
    "        return z\n",
    "    \n",
    "    # The 'activation function' sigmoid, which compresses the activation value to a value between 0 and 1.\n",
    "    \n",
    "    def sigmoid(self, z): \n",
    "        \"\"\" The output of a neuron \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "   \n",
    "    # We enter the activation value of the neurons into the sigmoid function to get the outputs. \n",
    "    # This specific function will compute the output of the neural network. \n",
    "    \n",
    "    def output(self, a):\n",
    "        return sigmoid(activation(a))\n",
    "    \n",
    "    ## The following functions will enable the output function to give a correct value. Our goal is to find the weights\n",
    "    ## and the biases that will lead to the correct output. To find this, we need to find how much the cost function \n",
    "    ## (i.e., the difference between the network's output and the correct output, as provided by the training data) \n",
    "    ## would change based on changes in the weights and the biases. \n",
    "    \n",
    "    # We start by finding the change in the cost function based on the changes in the output activations. \n",
    "    # How less wrong would we be if the final output activations were different? This information is the first \n",
    "    # step towards finding the change in the cost function for weights and biases, because it will be communicated\n",
    "    # back to the previous layers by 'backpropagation'. \n",
    "    \n",
    "    def cost_output_derivative(self, output_activations, y): # output_activations are created in backpropagation\n",
    "        \"\"\" Returns the partial derivates of the cost function with respect to all the output activations. \"\"\"\n",
    "        return output_activations - y\n",
    "    \n",
    "    def sigmoid_prime(z): # multiplying this with the derivative of the cost function we get an error term, which \n",
    "                          # tells us how much the output would change depending on the output activations. \n",
    "        \"\"\" Returns the derivative of the cost function. \"\"\"\n",
    "        return sigmoid(z) * (1-sigmoid(z))\n",
    "    \n",
    "    def backpropagation(self, x, y):\n",
    "        \"\"\" Returns the partial derivative of the cost function with respect to the weights and the biases. \n",
    "        The output is a tuple (grad_w, grad_b). The first element is a list of vectors, each vector containing \n",
    "        the partial derivatives of the cost function with respect to the weights in a single layer. The second \n",
    "        is the same with respect to biases. \"\"\"\n",
    "    \n",
    "    grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    \n",
    "    # first the information (the activations / outputs) is propagated forward via the weights and the biases \n",
    "    activation = x\n",
    "    activations = [x] # a list of vectors that will store all the activations in all the layers \n",
    "    zs = [] # the same for the weighted sums (the activation of an input neuron which has not been into the sigmoid yet)\n",
    "    for w, b in zip(self.biases, self.weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # propagating backwards: computing the error term\n",
    "    # calculated from the last layer, so index = -1\n",
    "    error = self.cost_output_derivative(activations[-1], y) * sigmoid_primte(zs[-1])\n",
    "    grad_w[-1] = np.dot(error, activations[-2].T)\n",
    "    grad_b[-1] = error \n",
    "    \n",
    "    for l in range(2, self.n_layers):\n",
    "        z = zs[-l]\n",
    "        sig_prime = sigmoid_prime(z)\n",
    "        error = np.dot(self.weights[-l + 1].T, error) * sig_prime\n",
    "        grad_w[-l] = np.dot(delta, activations[-l - 1].T))\n",
    "        grad_b[-l] = delta\n",
    "    return(grad_w, grad_b)\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta): # eta is just a constant that determines rate of learning. Do not concern\n",
    "                                                  # yourself with it. The 'mini_batch' needs some explanation. The data \n",
    "                                                  # consist of a list of tuples. The tuples (x, y) contain the inputs and the \n",
    "                                                  # desired outputs. A mini batch is a randomly selected sub sample of the data. \n",
    "                                                  # The gradient descent is calculated over these samples and then averaged. \n",
    "                                                  # It speeds up the process while giving accurate estimations of the gradient. \n",
    "                                                  # This function updates the weights and the biases for a single batch. It may \n",
    "                                                  # seem a bit mysterious at first because there is no code that actually \n",
    "                                                  # calculates the gradient. I will add a function for that in the following days. \n",
    "                                                  # It will be contained in the backprop() function, which is in this chunk.\n",
    "                                                  # I have not finished learning the back propagation algorithm yet but its \n",
    "                                                  # results should be used as specified here.\n",
    "                                                    \n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases] # vectors that will store the gradient for weights and biases\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights] # initially both are full of zeros. We will update them below.\n",
    "                                                           \n",
    "        for x, y in mini_batch: \n",
    "            delta_grad_b, delta_grad_w = self.backprop(x, y)\n",
    "            grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)] # accumulating change\n",
    "            grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)]\n",
    "\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, grad_b)] # subtract the average change in the \n",
    "                                                                                         # biases from the biases to update the biases                                                                      \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, grad_w)] # same for the weights \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]]\n",
      "[[3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2]])\n",
    "y = np.array([[3, 4]]).T\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 6],\n",
       "       [4, 8]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
